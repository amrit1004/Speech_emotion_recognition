{"metadata":{"colab":{"name":"parallel-cnn-attention-lstm.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Load file names","metadata":{"id":"acBn0jSXWmvJ"}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport librosa\nimport librosa.display\nimport IPython\nfrom IPython.display import Audio\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt\n\nEMOTIONS = {1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 0:'surprise'} # surprise je promenjen sa 8 na 0\nDATA_PATH = '../input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/'\nSAMPLE_RATE = 48000\n\ndata = pd.DataFrame(columns=['Emotion', 'Emotion intensity', 'Gender','Path'])\nfor dirname, _, filenames in os.walk(DATA_PATH):\n    for filename in filenames:\n        file_path = os.path.join('/kaggle/input/',dirname, filename)\n        identifiers = filename.split('.')[0].split('-')\n        emotion = (int(identifiers[2]))\n        if emotion == 8: # promeni surprise sa 8 na 0\n            emotion = 0\n        if int(identifiers[3]) == 1:\n            emotion_intensity = 'normal' \n        else:\n            emotion_intensity = 'strong'\n        if int(identifiers[6])%2 == 0:\n            gender = 'female'\n        else:\n            gender = 'male'\n        \n        data = data.append({\"Emotion\": emotion,\n                            \"Emotion intensity\": emotion_intensity,\n                            \"Gender\": gender,\n                            \"Path\": file_path\n                             },\n                             ignore_index = True\n                          )\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"L4isFoVsWmvN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"number of files is {}\".format(len(data)))\ndata.head()","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","id":"4ML1evkjWmvP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Number of examples per emotion","metadata":{"id":"CiwyLlFVWmvQ"}},{"cell_type":"code","source":"fig = plt.figure()\nax = fig.add_subplot(111)\nax.bar(x=range(8), height=data['Emotion'].value_counts())\nax.set_xticks(ticks=range(8))\nax.set_xticklabels([EMOTIONS[i] for i in range(8)],fontsize=10)\nax.set_xlabel('Emotions')\nax.set_ylabel('Number of examples')","metadata":{"id":"E-lc-Gh6WmvQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"number of examples per gender","metadata":{"id":"MIW_k_HBWmvR"}},{"cell_type":"code","source":"fig = plt.figure()\nax = fig.add_subplot(111)\ncounts = data['Gender'].value_counts()\nax.bar(x=[0,1], height=counts.values)\nax.set_xticks(ticks=[0,1])\nax.set_xticklabels(list(counts.index))\nax.set_xlabel('Gender')\nax.set_ylabel('Number of examples')","metadata":{"id":"AfeMgcr7WmvS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"number of examples per emotion intensity","metadata":{"id":"vuTqjg8BWmvS"}},{"cell_type":"code","source":"fig = plt.figure()\nax = fig.add_subplot(111)\ncounts = data['Emotion intensity'].value_counts()\nax.bar(x=[0,1], height=counts.values)\nax.set_xticks(ticks=[0,1])\nax.set_xticklabels(list(counts.index))\nax.set_xlabel('Gender')\nax.set_ylabel('Number of examples')","metadata":{"id":"bwC_JGI7WmvT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load the signals","metadata":{"id":"aYePgCseWmvU"}},{"cell_type":"code","source":"mel_spectrograms = []\nsignals = []\nfor i, file_path in enumerate(data.Path):\n    audio, sample_rate = librosa.load(file_path, duration=3, offset=0.5, sr=SAMPLE_RATE)\n    signal = np.zeros((int(SAMPLE_RATE*3,)))\n    signal[:len(audio)] = audio\n    signals.append(signal)\n    print(\"\\r Processed {}/{} files\".format(i,len(data)),end='')\nsignals = np.stack(signals,axis=0)","metadata":{"id":"_8g9QufkWmvU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split the data","metadata":{"id":"QVW1Av6BWmvV"}},{"cell_type":"code","source":"X = signals\ntrain_ind,test_ind,val_ind = [],[],[]\nX_train,X_val,X_test = [],[],[]\nY_train,Y_val,Y_test = [],[],[]\nfor emotion in range(len(EMOTIONS)):\n    emotion_ind = list(data.loc[data.Emotion==emotion,'Emotion'].index)\n    emotion_ind = np.random.permutation(emotion_ind)\n    m = len(emotion_ind)\n    ind_train = emotion_ind[:int(0.8*m)]\n    ind_val = emotion_ind[int(0.8*m):int(0.9*m)]\n    ind_test = emotion_ind[int(0.9*m):]\n    X_train.append(X[ind_train,:])\n    Y_train.append(np.array([emotion]*len(ind_train),dtype=np.int32))\n    X_val.append(X[ind_val,:])\n    Y_val.append(np.array([emotion]*len(ind_val),dtype=np.int32))\n    X_test.append(X[ind_test,:])\n    Y_test.append(np.array([emotion]*len(ind_test),dtype=np.int32))\n    train_ind.append(ind_train)\n    test_ind.append(ind_test)\n    val_ind.append(ind_val)\nX_train = np.concatenate(X_train,0)\nX_val = np.concatenate(X_val,0)\nX_test = np.concatenate(X_test,0)\nY_train = np.concatenate(Y_train,0)\nY_val = np.concatenate(Y_val,0)\nY_test = np.concatenate(Y_test,0)\ntrain_ind = np.concatenate(train_ind,0)\nval_ind = np.concatenate(val_ind,0)\ntest_ind = np.concatenate(test_ind,0)\nprint(f'X_train:{X_train.shape}, Y_train:{Y_train.shape}')\nprint(f'X_val:{X_val.shape}, Y_val:{Y_val.shape}')\nprint(f'X_test:{X_test.shape}, Y_test:{Y_test.shape}')\n# check if all are unique\nunique, count = np.unique(np.concatenate([train_ind,test_ind,val_ind],0), return_counts=True)\nprint(\"Number of unique indexes is {}, out of {}\".format(sum(count==1), X.shape[0]))\n\ndel X","metadata":{"id":"qXduocCHWmvV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Augment signals by adding AWGN","metadata":{"id":"KQgfbrsXWmvW"}},{"cell_type":"code","source":"def addAWGN(signal, num_bits=16, augmented_num=2, snr_low=15, snr_high=30): \n    signal_len = len(signal)\n    # Generate White Gaussian noise\n    noise = np.random.normal(size=(augmented_num, signal_len))\n    # Normalize signal and noise\n    norm_constant = 2.0**(num_bits-1)\n    signal_norm = signal / norm_constant\n    noise_norm = noise / norm_constant\n    # Compute signal and noise power\n    s_power = np.sum(signal_norm ** 2) / signal_len\n    n_power = np.sum(noise_norm ** 2, axis=1) / signal_len\n    # Random SNR: Uniform [15, 30] in dB\n    target_snr = np.random.randint(snr_low, snr_high)\n    # Compute K (covariance matrix) for each noise \n    K = np.sqrt((s_power / n_power) * 10 ** (- target_snr / 10))\n    K = np.ones((signal_len, augmented_num)) * K  \n    # Generate noisy signal\n    return signal + K.T * noise","metadata":{"id":"RbhKUl6kWmvX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aug_signals = []\naug_labels = []\nfor i in range(X_train.shape[0]):\n    signal = X_train[i,:]\n    augmented_signals = addAWGN(signal)\n    for j in range(augmented_signals.shape[0]):\n        aug_labels.append(data.loc[i,\"Emotion\"])\n        aug_signals.append(augmented_signals[j,:])\n        data = data.append(data.iloc[i], ignore_index=True)\n    print(\"\\r Processed {}/{} files\".format(i,X_train.shape[0]),end='')\naug_signals = np.stack(aug_signals,axis=0)\nX_train = np.concatenate([X_train,aug_signals],axis=0)\naug_labels = np.stack(aug_labels,axis=0)\nY_train = np.concatenate([Y_train,aug_labels])\nprint('')\nprint(f'X_train:{X_train.shape}, Y_train:{Y_train.shape}')","metadata":{"id":"X1toUPgkWmvY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Calculate mel spectrograms","metadata":{"id":"vM2L_HIrWmvZ"}},{"cell_type":"code","source":"def getMELspectrogram(audio, sample_rate):\n    mel_spec = librosa.feature.melspectrogram(y=audio,\n                                              sr=sample_rate,\n                                              n_fft=1024,\n                                              win_length = 512,\n                                              window='hamming',\n                                              hop_length = 256,\n                                              n_mels=128,\n                                              fmax=sample_rate/2\n                                             )\n    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n    return mel_spec_db\n\n# test function\naudio, sample_rate = librosa.load(data.loc[0,'Path'], duration=3, offset=0.5,sr=SAMPLE_RATE)\nsignal = np.zeros((int(SAMPLE_RATE*3,)))\nsignal[:len(audio)] = audio\nmel_spectrogram = getMELspectrogram(signal, SAMPLE_RATE)\nlibrosa.display.specshow(mel_spectrogram, y_axis='mel', x_axis='time')\nprint('MEL spectrogram shape: ',mel_spectrogram.shape)","metadata":{"id":"SjOQZFmUWmvZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mel_train = []\nprint(\"Calculatin mel spectrograms for train set\")\nfor i in range(X_train.shape[0]):\n    mel_spectrogram = getMELspectrogram(X_train[i,:], sample_rate=SAMPLE_RATE)\n    mel_train.append(mel_spectrogram)\n    print(\"\\r Processed {}/{} files\".format(i,X_train.shape[0]),end='')\nprint('')\nmel_train = np.stack(mel_train,axis=0)\ndel X_train\nX_train = mel_train\n\nmel_val = []\nprint(\"Calculatin mel spectrograms for val set\")\nfor i in range(X_val.shape[0]):\n    mel_spectrogram = getMELspectrogram(X_val[i,:], sample_rate=SAMPLE_RATE)\n    mel_val.append(mel_spectrogram)\n    print(\"\\r Processed {}/{} files\".format(i,X_val.shape[0]),end='')\nprint('')\nmel_val = np.stack(mel_val,axis=0)\ndel X_val\nX_val = mel_val\n\nmel_test = []\nprint(\"Calculatin mel spectrograms for test set\")\nfor i in range(X_test.shape[0]):\n    mel_spectrogram = getMELspectrogram(X_test[i,:], sample_rate=SAMPLE_RATE)\n    mel_test.append(mel_spectrogram)\n    print(\"\\r Processed {}/{} files\".format(i,X_test.shape[0]),end='')\nprint('')\nmel_test = np.stack(mel_test,axis=0)\ndel X_test\nX_test = mel_test\n\nprint(f'X_train:{X_train.shape}, Y_train:{Y_train.shape}')\nprint(f'X_val:{X_val.shape}, Y_val:{Y_val.shape}')\nprint(f'X_test:{X_test.shape}, Y_test:{Y_test.shape}')","metadata":{"id":"vsu9C-Z_Wmva"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create the model","metadata":{"id":"E71VeUp5Wmvb"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass ParallelModel(nn.Module):\n    def __init__(self,num_emotions):\n        super().__init__()\n        # conv block\n        self.conv2Dblock = nn.Sequential(\n            # 1. conv block\n            nn.Conv2d(in_channels=1,\n                       out_channels=16,\n                       kernel_size=3,\n                       stride=1,\n                       padding=1\n                      ),\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Dropout(p=0.3),\n            # 2. conv block\n            nn.Conv2d(in_channels=16,\n                       out_channels=32,\n                       kernel_size=3,\n                       stride=1,\n                       padding=1\n                      ),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=4, stride=4),\n            nn.Dropout(p=0.3),\n            # 3. conv block\n            nn.Conv2d(in_channels=32,\n                       out_channels=64,\n                       kernel_size=3,\n                       stride=1,\n                       padding=1\n                      ),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=4, stride=4),\n            nn.Dropout(p=0.3),\n            # 4. conv block\n            nn.Conv2d(in_channels=64,\n                       out_channels=64,\n                       kernel_size=3,\n                       stride=1,\n                       padding=1\n                      ),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=4, stride=4),\n            nn.Dropout(p=0.3)\n        )\n        # LSTM block\n        self.lstm_maxpool = nn.MaxPool2d(kernel_size=[2,4], stride=[2,4])\n        hidden_size = 128\n        self.lstm = nn.LSTM(input_size=64,hidden_size=hidden_size,bidirectional=True, batch_first=True)\n        self.dropout_lstm = nn.Dropout(0.1)\n        self.attention_linear = nn.Linear(2*hidden_size,1) # 2*hidden_size for the 2 outputs of bidir LSTM\n        # Linear softmax layer\n        self.out_linear = nn.Linear(2*hidden_size+256,num_emotions)\n        self.dropout_linear = nn.Dropout(p=0)\n        self.out_softmax = nn.Softmax(dim=1)\n    def forward(self,x):\n        # conv embedding\n        conv_embedding = self.conv2Dblock(x) #(b,channel,freq,time)\n        conv_embedding = torch.flatten(conv_embedding, start_dim=1) # do not flatten batch dimension\n        # lstm embedding\n        x_reduced = self.lstm_maxpool(x)\n        x_reduced = torch.squeeze(x_reduced,1)\n        x_reduced = x_reduced.permute(0,2,1) # (b,t,freq)\n        lstm_embedding, (h,c) = self.lstm(x_reduced) # (b, time, hidden_size*2)\n        lstm_embedding = self.dropout_lstm(lstm_embedding)\n        batch_size,T,_ = lstm_embedding.shape \n        attention_weights = [None]*T\n        for t in range(T):\n            embedding = lstm_embedding[:,t,:]\n            attention_weights[t] = self.attention_linear(embedding)\n        attention_weights_norm = nn.functional.softmax(torch.stack(attention_weights,-1),-1)\n        attention = torch.bmm(attention_weights_norm,lstm_embedding) # (Bx1xT)*(B,T,hidden_size*2)=(B,1,2*hidden_size)\n        attention = torch.squeeze(attention, 1)\n        # concatenate\n        complete_embedding = torch.cat([conv_embedding, attention], dim=1) \n        \n        output_logits = self.out_linear(complete_embedding)\n        output_logits = self.dropout_linear(output_logits)\n        output_softmax = self.out_softmax(output_logits)\n        return output_logits, output_softmax, attention_weights_norm\n                                     ","metadata":{"id":"Cb5FUckmWmvc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def loss_fnc(predictions, targets):\n    return nn.CrossEntropyLoss()(input=predictions,target=targets)","metadata":{"id":"8w51AKkUWmvd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TRAINING","metadata":{"id":"10sxtTuNWmvd"}},{"cell_type":"code","source":"def make_train_step(model, loss_fnc, optimizer):\n    def train_step(X,Y):\n        # set model to train mode\n        model.train()\n        # forward pass\n        output_logits, output_softmax, attention_weights_norm = model(X)\n        predictions = torch.argmax(output_softmax,dim=1)\n        accuracy = torch.sum(Y==predictions)/float(len(Y))\n        # compute loss\n        loss = loss_fnc(output_logits, Y)\n        # compute gradients\n        loss.backward()\n        # update parameters and zero gradients\n        optimizer.step()\n        optimizer.zero_grad()\n        return loss.item(), accuracy*100\n    return train_step","metadata":{"id":"EE987u5qWmvd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_validate_fnc(model,loss_fnc):\n    def validate(X,Y):\n        with torch.no_grad():\n            model.eval()\n            output_logits, output_softmax, attention_weights_norm = model(X)\n            predictions = torch.argmax(output_softmax,dim=1)\n            accuracy = torch.sum(Y==predictions)/float(len(Y))\n            loss = loss_fnc(output_logits,Y)\n        return loss.item(), accuracy*100, predictions\n    return validate","metadata":{"id":"pP0GGToIWmve"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"scale data","metadata":{"id":"xbTE4Rr_Wmve"}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nX_train = np.expand_dims(X_train,1)\nX_val = np.expand_dims(X_val,1)\nX_test = np.expand_dims(X_test,1)\n\nscaler = StandardScaler()\n\nb,c,h,w = X_train.shape\nX_train = np.reshape(X_train, newshape=(b,-1))\nX_train = scaler.fit_transform(X_train)\nX_train = np.reshape(X_train, newshape=(b,c,h,w))\n\nb,c,h,w = X_test.shape\nX_test = np.reshape(X_test, newshape=(b,-1))\nX_test = scaler.transform(X_test)\nX_test = np.reshape(X_test, newshape=(b,c,h,w))\n\nb,c,h,w = X_val.shape\nX_val = np.reshape(X_val, newshape=(b,-1))\nX_val = scaler.transform(X_val)\nX_val = np.reshape(X_val, newshape=(b,c,h,w))","metadata":{"id":"Nqy3qHcoWmve"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train the model","metadata":{"id":"tDN-lRs7Wmvf"}},{"cell_type":"code","source":"EPOCHS=1500\nDATASET_SIZE = X_train.shape[0]\nBATCH_SIZE = 32\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint('Selected device is {}'.format(device))\nmodel = ParallelModel(num_emotions=len(EMOTIONS)).to(device)\nprint('Number of trainable params: ',sum(p.numel() for p in model.parameters()) )\nOPTIMIZER = torch.optim.SGD(model.parameters(),lr=0.01, weight_decay=1e-3, momentum=0.8)\n\ntrain_step = make_train_step(model, loss_fnc, optimizer=OPTIMIZER)\nvalidate = make_validate_fnc(model,loss_fnc)\nlosses=[]\nval_losses = []\nfor epoch in range(EPOCHS):\n    # schuffle data\n    ind = np.random.permutation(DATASET_SIZE)\n    X_train = X_train[ind,:,:,:]\n    Y_train = Y_train[ind]\n    epoch_acc = 0\n    epoch_loss = 0\n    iters = int(DATASET_SIZE / BATCH_SIZE)\n    for i in range(iters):\n        batch_start = i * BATCH_SIZE\n        batch_end = min(batch_start + BATCH_SIZE, DATASET_SIZE)\n        actual_batch_size = batch_end-batch_start\n        X = X_train[batch_start:batch_end,:,:,:]\n        Y = Y_train[batch_start:batch_end]\n        X_tensor = torch.tensor(X,device=device).float()\n        Y_tensor = torch.tensor(Y, dtype=torch.long,device=device)\n        loss, acc = train_step(X_tensor,Y_tensor)\n        epoch_acc += acc*actual_batch_size/DATASET_SIZE\n        epoch_loss += loss*actual_batch_size/DATASET_SIZE\n        print(f\"\\r Epoch {epoch}: iteration {i}/{iters}\",end='')\n    X_val_tensor = torch.tensor(X_val,device=device).float()\n    Y_val_tensor = torch.tensor(Y_val,dtype=torch.long,device=device)\n    val_loss, val_acc, _ = validate(X_val_tensor,Y_val_tensor)\n    losses.append(epoch_loss)\n    val_losses.append(val_loss)\n    print('')\n    print(f\"Epoch {epoch} --> loss:{epoch_loss:.4f}, acc:{epoch_acc:.2f}%, val_loss:{val_loss:.4f}, val_acc:{val_acc:.2f}%\")\n    ","metadata":{"id":"CRbGiqWuWmvf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save model","metadata":{"id":"xwysi2poWmvf"}},{"cell_type":"code","source":"SAVE_PATH = os.path.join(os.getcwd(),'models')\nos.makedirs('models',exist_ok=True)\ntorch.save(model.state_dict(),os.path.join(SAVE_PATH,'cnn_lstm_parallel_model.pt'))\nprint('Model is saved to {}'.format(os.path.join(SAVE_PATH,'cnn_lstm_parallel_model.pt')))","metadata":{"id":"KbJcOvkUWmvg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load model","metadata":{"id":"ltVwqKGpWmvg"}},{"cell_type":"code","source":"LOAD_PATH = os.path.join(os.getcwd(),'models')\nmodel = ParallelModel(len(EMOTIONS))\nmodel.load_state_dict(torch.load(os.path.join(LOAD_PATH,'cnn_lstm_parallel_model.pt')))\nprint('Model is loaded from {}'.format(os.path.join(LOAD_PATH,'cnn_lstm_parallel_model.pt')))","metadata":{"id":"EKXnTrlZWmvg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test","metadata":{"id":"Y-OAi2f0Wmvg"}},{"cell_type":"code","source":"X_test_tensor = torch.tensor(X_test,device=device).float()\nY_test_tensor = torch.tensor(Y_test,dtype=torch.long,device=device)\ntest_loss, test_acc, predictions = validate(X_test_tensor,Y_test_tensor)\nprint(f'Test loss is {test_loss:.3f}')\nprint(f'Test accuracy is {test_acc:.2f}%')","metadata":{"id":"AXIYDQajWmvh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"confusion matrix","metadata":{"id":"UoAen3PtWmvh"}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sn\n\npredictions = predictions.cpu().numpy()\ncm = confusion_matrix(Y_test, predictions)\nnames = [EMOTIONS[ind] for ind in range(len(EMOTIONS))]\ndf_cm = pd.DataFrame(cm, index=names, columns=names)\n# plt.figure(figsize=(10,7))\nsn.set(font_scale=1.4) # for label size\nsn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\nplt.show()","metadata":{"id":"ktWIKtY8Wmvh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"correlation between emotion intensity and corectness","metadata":{"id":"LAAfoCzLWmvh"}},{"cell_type":"code","source":"correct_strong = 0\ncorrect_normal = 0\nwrong_strong = 0\nwrong_normal = 0\nfor i in range(len(X_test)):\n    intensity = data.loc[test_ind[i],'Emotion intensity']\n    if Y_test[i] == predictions[i]: # correct prediction\n        if  intensity == 'normal':\n            correct_normal += 1\n        else:\n            correct_strong += 1\n    else: # wrong prediction\n        if intensity == 'normal':\n            wrong_normal += 1\n        else:\n            wrong_strong += 1\narray = np.array([[wrong_normal,wrong_strong],[correct_normal,correct_strong]])\ndf = pd.DataFrame(array,['wrong','correct'],['normal','strong'])\nsn.set(font_scale=1.4) # for label size\nsn.heatmap(df, annot=True, annot_kws={\"size\": 16}) # font size\nplt.show()","metadata":{"id":"T4qYCNpNWmvi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"correlation between gender and corectness","metadata":{"id":"pYzjN196Wmvi"}},{"cell_type":"code","source":"correct_male = 0\ncorrect_female = 0\nwrong_male = 0\nwrong_female = 0\nfor i in range(len(X_test)):\n    gender = data.loc[test_ind[i],'Gender']\n    if Y_test[i] == predictions[i]: # correct prediction\n        if  gender == 'male':\n            correct_male += 1\n        else:\n            correct_female += 1\n    else: # wrong prediction\n        if gender == 'male':\n            wrong_male += 1\n        else:\n            wrong_female += 1\narray = np.array([[wrong_male,wrong_female],[correct_male,correct_female]])\ndf = pd.DataFrame(array,['wrong','correct'],['male','female'])\nsn.set(font_scale=1.4) # for label size\nsn.heatmap(df, annot=True, annot_kws={\"size\": 16}) # font size\nplt.show()","metadata":{"id":"N7BKkR1WWmvi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot loss","metadata":{"id":"3KfDrZcCWmvi"}},{"cell_type":"code","source":"plt.plot(losses,'b')\nplt.plot(val_losses,'r')\nplt.legend(['train loss','val loss'])","metadata":{"id":"m2EuoqsAWmvj"},"execution_count":null,"outputs":[]}]}